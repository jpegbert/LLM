{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d579d393-d30d-4dda-9cb0-657c86e6f1d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4f4b15-318a-4f4f-8a66-d91385906952",
   "metadata": {},
   "source": [
    "## 1. åŠ è½½æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d447a5-22dc-49b7-8d26-16b99bf721eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for gem/viggo contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/gem/viggo\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.14k/3.14k [00:00<00:00, 4.40MB/s]\n",
      "Downloading metadata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.81k/3.81k [00:00<00:00, 4.82MB/s]\n",
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.5k/24.5k [00:00<00:00, 674kB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.37M/1.37M [00:00<00:00, 25.2MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187k/187k [00:00<00:00, 22.3MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 269k/269k [00:00<00:00, 22.2MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11.2k/11.2k [00:00<00:00, 9.43MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.1k/25.1k [00:00<00:00, 706kB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64.3k/64.3k [00:00<00:00, 222kB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123k/123k [00:00<00:00, 271kB/s] \n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 253k/253k [00:00<00:00, 371kB/s]  \n",
      "Generating train split: 5103 examples [00:00, 15092.28 examples/s]\n",
      "Generating validation split: 714 examples [00:00, 18331.32 examples/s]\n",
      "Generating test split: 1083 examples [00:00, 18574.20 examples/s]\n",
      "Generating challenge_train_1_percent split: 50 examples [00:00, 9273.69 examples/s]\n",
      "Generating challenge_train_2_percent split: 103 examples [00:00, 12654.17 examples/s]\n",
      "Generating challenge_train_5_percent split: 256 examples [00:00, 16216.48 examples/s]\n",
      "Generating challenge_train_10_percent split: 510 examples [00:00, 17904.87 examples/s]\n",
      "Generating challenge_train_20_percent split: 1021 examples [00:00, 18632.11 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['gem_id', 'meaning_representation', 'target', 'references'],\n",
      "    num_rows: 5103\n",
      "})\n",
      "Dataset({\n",
      "    features: ['gem_id', 'meaning_representation', 'target', 'references'],\n",
      "    num_rows: 714\n",
      "})\n",
      "Dataset({\n",
      "    features: ['gem_id', 'meaning_representation', 'target', 'references'],\n",
      "    num_rows: 1083\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"gem/viggo\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"gem/viggo\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"gem/viggo\", split=\"test\")\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0db785-8df3-478b-a0bf-8ad6c64f7977",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gem_id': 'viggo-train-0',\n",
       " 'meaning_representation': 'inform(name[Dirt: Showdown], release_year[2012], esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport], platforms[PlayStation, Xbox, PC], available_on_steam[no], has_linux_release[no], has_mac_release[no])',\n",
       " 'target': \"Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\",\n",
       " 'references': [\"Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac.\"]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æŸ¥çœ‹æ ·æœ¬\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421faab-4303-4a9f-8aa0-8592e269c687",
   "metadata": {},
   "source": [
    "## 2. åŠ è½½åŸºæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b32d94-3988-4251-a353-21ff300076ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_4bit_use_double_type']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# llama-3-8B\n",
    "base_model_id = \"/root/autodl-tmp/model/Meta-Llama-3-8B\"\n",
    "\n",
    "# é‡åŒ–é…ç½®\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_type = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id,\n",
    "                                             quantization_config = bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a0b74d-bb86-4f87-9479-70555f897c21",
   "metadata": {},
   "source": [
    "## 3. åŠ è½½ Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296518a3-fc5f-4dd5-bd64-af4982deed71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    model_max_length = 512,\n",
    "    padding_side = \"left\",\n",
    "    add_eos_token = True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d24ffab-8052-4f27-b14e-dc6e2181f877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    '''åˆ†è¯å™¨'''\n",
    "    result = tokenizer(prompt,\n",
    "                       truncation = True,\n",
    "                       max_length = 512,\n",
    "                       padding = \"max_length\")\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87c8f8fa-4ae5-4ba2-846d-ed15f761f0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt =f\"\"\"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
    "                    This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
    "                    The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
    "                    \n",
    "                    ### Target sentence:\n",
    "                    {data_point[\"target\"]}\n",
    "                    \n",
    "                    ### Meaning representation:\n",
    "                    {data_point[\"meaning_representation\"]}\n",
    "                  \"\"\"\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39551011-1bbc-4ecb-a477-1fa85a0f8aac",
   "metadata": {},
   "source": [
    "## 4. å¯¹trainå’Œevalæ•°æ®é›†è¿›è¡Œtokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c6a506e-8172-489b-a78e-2c9d85095af8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5103/5103 [00:02<00:00, 1924.83 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 714/714 [00:00<00:00, 1946.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa431daa-4fab-45d1-9b1b-3ddd6c87f458",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128000, 22818, 264, 2218, 11914, 9429, 279, 16940, 7438, 13340, 315, 279, 1988, 11914, 439, 264, 3254, 734, 449, 8365, 323, 7180, 2819, 627, 504, 1115, 734, 1288, 7664, 279, 2218, 925, 30357, 323, 279, 734, 2011, 387, 832, 315, 279, 2768, 2570, 41540, 518, 364, 2079, 518, 364, 47530, 10499, 37400, 518, 364, 14119, 518, 364, 12728, 17209, 518, 364, 96861, 518, 364, 2079, 2769, 36990, 518, 364, 67689, 518, 364, 2079, 17209, 663, 627, 504, 578, 8365, 2011, 387, 832, 315, 279, 2768, 25, 2570, 609, 518, 364, 4683, 25596, 4257, 518, 364, 23859, 14987, 518, 364, 35501, 518, 364, 288, 10910, 518, 364, 22696, 518, 364, 65011, 518, 364, 3517, 623, 86191, 518, 364, 4752, 26190, 3517, 518, 364, 16111, 82, 518, 364, 10547, 4570, 1284, 14922, 518, 364, 4752, 78563, 25596, 518, 364, 4752, 23647, 25596, 518, 364, 68351, 4532, 10912, 504, 17010, 13791, 11914, 512, 504, 87419, 25, 7073, 2996, 374, 264, 10775, 22019, 1847, 430, 574, 6004, 304, 220, 679, 17, 13, 578, 1847, 374, 2561, 389, 32365, 11, 21222, 11, 323, 6812, 11, 323, 433, 706, 459, 469, 14899, 33, 19767, 315, 469, 220, 605, 10, 320, 2000, 22172, 220, 605, 323, 55025, 570, 4452, 11, 433, 374, 539, 3686, 2561, 439, 264, 22578, 11, 14677, 11, 477, 7553, 4984, 627, 10912, 504, 17010, 49203, 13340, 512, 504, 6179, 3232, 66020, 2154, 25, 7073, 2996, 1145, 4984, 14987, 58, 679, 17, 1145, 1560, 10910, 61097, 220, 605, 10, 320, 2000, 22172, 220, 605, 323, 55025, 26090, 36744, 58, 3696, 2299, 7534, 4628, 11, 10775, 1145, 15771, 58, 9315, 20348, 11, 21222, 11, 6812, 1145, 2561, 4570, 1284, 14922, 58, 2201, 1145, 706, 78563, 25596, 58, 2201, 1145, 706, 23647, 25596, 58, 2201, 2608, 4391]\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹æ ·æœ¬\n",
    "\n",
    "print(tokenized_train_dataset[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c320d32f-d8fb-455e-9398-038c710476b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_train_dataset[1][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9f81a-dfd8-4180-89bc-8d9f96ecfef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ea0167e-6949-434d-b1b8-e97f7d78251d",
   "metadata": {},
   "source": [
    "## 5. åŸºäº base model è¿›è¡Œæµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44955ef1-739d-4006-9fda-5a63c9fc7642",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç›®æ ‡è¯­å¥: \n",
      " Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?\n",
      "æ„ä¹‰è¡¨ç¤º: \n",
      " verify_attribute(name[Little Big Adventure], rating[average], has_multiplayer[no], platforms[PlayStation])\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹ test æ•°æ®é›†æ ·æœ¬\n",
    "\n",
    "print(\"ç›®æ ‡è¯­å¥: \\n\", test_dataset[1][\"target\"])\n",
    "\n",
    "print(\"æ„ä¹‰è¡¨ç¤º: \\n\", test_dataset[1][\"meaning_representation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eb03a6a-9038-481b-bc1a-4ee500fbc4a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
    "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
    "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
    "\n",
    "### Target sentence:\n",
    "Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?\n",
    "\n",
    "### Meaning representation:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d049ec84-caaf-4572-822b-3e70868d1431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2024-05-24 09:41:33.731816: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-24 09:41:33.772823: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-24 09:41:34.419360: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
      "This function should describe the target string accurately and the function must be one of the following ['inform','request', 'give_opinion', 'confirm','verify_attribute','suggest','request_explanation','recommend','request_attribute'].\n",
      "The attributes must be one of the following: ['name', 'exp_release_date','release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release','specifier']\n",
      "\n",
      "### Target sentence:\n",
      "Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?\n",
      "\n",
      "### Meaning representation:\n",
      "{'type': 'inform', 'attributes': [{'name': 'opinion', 'value': 'true'}], 'attributes_values': [{'name': 'opinion', 'value': 'true'}], 'attributes_values': [{'name': 'has_multiplayer', 'value': 'false'}]}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# é‡æ–°åˆå§‹åŒ– tokenizerï¼Œè¿™æ ·å®ƒå°±ä¸ä¼šæ·»åŠ  padding æˆ– eos token\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token = True,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # æ¨¡å‹æ¨ç†\n",
    "    result = model.generate(**model_input, max_new_tokens=256)\n",
    "    # è§£ç \n",
    "    result = eval_tokenizer.decode(result[0], skip_special_tokens=True)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5672b-c1fa-40a4-ac57-04cc91fc1862",
   "metadata": {},
   "source": [
    "## 6. é…ç½® LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60310901-1a48-4f40-9e03-564afef842de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ca26948-60b5-4d81-a1d0-e774d7b6bdc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    '''è®¡ç®—è®­ç»ƒçš„å‚æ•°é‡'''\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"è®­ç»ƒå‚æ•°é‡ï¼š{trainable_params} || æ‰€æœ‰å‚æ•°é‡ï¼š{all_param} || å¯è®­ç»ƒå‚æ•°é‡æ¯”ä¾‹ï¼š{100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ab1d475-09e0-49f5-8dda-aa40aa286739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# æ‰“å°æ¨¡å‹\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbcb5062-ea27-401d-8abd-017df2c24bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒå‚æ•°é‡ï¼š22030336 || æ‰€æœ‰å‚æ•°é‡ï¼š4562630656 || å¯è®­ç»ƒå‚æ•°é‡æ¯”ä¾‹ï¼š0.4828428523143645\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha = 16,\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias = \"none\",\n",
    "    lora_dropout = 0.05,\n",
    "    task_type = \"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fcaf80c-7dc8-47cf-8473-78e93f6bd09b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# æ‰“å°æ¨¡å‹\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c522c151-2a36-4870-9d44-5765db42a13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "672b59a3-cd0a-4d27-8934-c702fb7a828d",
   "metadata": {},
   "source": [
    "## 7. wandb é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e928dc7-afd0-47a6-88dc-ab589042349f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtommytang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# éœ€è¦åœ¨WandBå®˜ç½‘æ³¨å†Œè´¦å·\n",
    "import wandb\n",
    "\n",
    "wandb.login(key=\"11a0ff012b65b101fdf6613d7c21f66a5960e623\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76195288-2a9a-47ff-8faa-0a0b1a22ac65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/autodl-tmp/project/2_QLoRA/notebook/wandb/run-20240524_094140-pt3l477p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tommytang/llama-3-8B-QLoRA/runs/pt3l477p' target=\"_blank\">twilight-sun-7</a></strong> to <a href='https://wandb.ai/tommytang/llama-3-8B-QLoRA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tommytang/llama-3-8B-QLoRA' target=\"_blank\">https://wandb.ai/tommytang/llama-3-8B-QLoRA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tommytang/llama-3-8B-QLoRA/runs/pt3l477p' target=\"_blank\">https://wandb.ai/tommytang/llama-3-8B-QLoRA/runs/pt3l477p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"llama-3-8B-QLoRA\",\n",
    "    job_type = \"training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a82474-3f80-492f-9f5f-b22564e96a81",
   "metadata": {},
   "source": [
    "## 8. æ¨¡å‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6e9e97a-cf39-49c8-b1ae-6610026885fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73a4c520-17fe-47b1-98ec-2a6a654db3f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "run_name = \"llama-3-8B-QLoRA\"\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    # æŒ‡å®šè¦è®­ç»ƒçš„æ¨¡å‹\n",
    "    model = model,\n",
    "    # æŒ‡å®šè®­ç»ƒæ•°æ®é›†\n",
    "    train_dataset = tokenized_train_dataset,\n",
    "    # æŒ‡å®šéªŒè¯æ•°æ®é›†\n",
    "    eval_dataset = tokenized_val_dataset,\n",
    "    # è®­ç»ƒå‚æ•°é…ç½®\n",
    "    args = transformers.TrainingArguments(\n",
    "        output_dir = output_dir, # è®­ç»ƒè¾“å‡ºçš„ç›®å½•\n",
    "        warmup_steps = 5, # è®­ç»ƒè¿‡ç¨‹ä¸­çš„é¢„çƒ­æ­¥éª¤æ•° \n",
    "        # è§£é‡Šï¼š\n",
    "        # warmup_steps ï¼šåœ¨è®­ç»ƒçš„åˆå§‹é˜¶æ®µï¼Œå­¦ä¹ ç‡ä»ä¸€ä¸ªè¾ƒä½çš„å€¼é€æ­¥å¢åŠ åˆ°è®¾å®šçš„å­¦ä¹ ç‡ã€‚\n",
    "        #                é¢„çƒ­æ­¥éª¤çš„ä½œç”¨æ˜¯é¿å…æ¨¡å‹åœ¨ä¸€å¼€å§‹å°±æ”¶åˆ°è¾ƒå¤§çš„æ¢¯åº¦æ›´æ–°ï¼Œä»è€Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚\n",
    "        per_device_train_batch_size = 4, # è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
    "        gradient_checkpointing = True, # æ˜¯å¦å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜\n",
    "        # è§£é‡Šï¼š\n",
    "        # gradient_checkpointing ï¼š è¿™æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå…è®¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­èŠ‚çœæ˜¾å­˜ã€‚\n",
    "        #                          å…·ä½“æ¥è¯´ï¼Œå®ƒä¼šåœ¨å‰å‘ä¼ æ’­æ—¶ä¿å­˜æŸäº›ä¸­é—´ç»“æœï¼Œè€Œä¸æ˜¯æ‰€æœ‰ä¸­é—´ç»“æœï¼Œä»è€Œå‡å°‘æ˜¾å­˜å ç”¨é‡ã€‚ç„¶ååœ¨åå‘ä¼ æ’­æ—¶ï¼Œå¿…è¦æ—¶é‡æ–°è®¡ç®—è¿™äº›ä¸­é—´ç»“æœã€‚\n",
    "        gradient_accumulation_steps = 4, # æ¢¯åº¦ç´¯ç§¯çš„æ­¥æ•°ï¼Œå®é™… batch size = per_device_train_batch_size * gradient_accumulation_steps\n",
    "        max_steps = 500, # æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼Œ1000,5000ç­‰\n",
    "        learning_rate = 2.5e-5, # å­¦ä¹ ç‡\n",
    "        logging_steps = 50, # æ¯ 50 æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—\n",
    "        bf16 = True, # ä½¿ç”¨ bfloat16 ç²¾åº¦è¿›è¡Œè®­ç»ƒ\n",
    "        optim = \"paged_adamw_8bit\", # ä½¿ç”¨8-bitçš„AdamWä¼˜åŒ–å™¨\n",
    "        # è§£é‡Šï¼š\n",
    "        # paged_adamw_8bit ï¼šè¿™æ˜¯ä¸€ç§ä¼˜åŒ–å™¨çš„å®ç°ï¼Œå°†å‚æ•°å’Œæ¢¯åº¦å‹ç¼©åˆ°8-bitè¡¨ç¤ºï¼Œä»¥å‡å°‘å†…å­˜å’Œè®¡ç®—éœ€æ±‚ã€‚\n",
    "        #                    Pagedè¡¨ç¤ºåˆ™æ˜¯æŒ‡ä¼˜åŒ–å™¨åˆ†é¡µå¤„ç†æ•°æ®ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ã€‚\n",
    "        logging_dir = \"./logs\", # æ—¥å¿—å­˜å‚¨ç›®å½•\n",
    "        save_strategy = \"steps\", # æ¨¡å‹ä¿å­˜ç­–ç•¥ï¼šæ¯éš”ä¸€å®šæ­¥æ•°ä¿å­˜ä¸€æ¬¡\n",
    "        save_steps = 50, # æ¯50æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹æ£€æŸ¥ç‚¹\n",
    "        evaluation_strategy = \"steps\", # è¯„ä¼°ç­–ç•¥ï¼šæ¯éš”ä¸€å®šæ­¥æ•°è¿›è¡Œè¯„ä¼°\n",
    "        eval_steps = 50, # æ¯50æ­¥è¿›è¡Œä¸€æ¬¡è¯„ä¼°\n",
    "        do_eval = True, # æ˜¯å¦åœ¨è®­ç»ƒç»“æŸåè¿›è¡Œè¯„ä¼°\n",
    "        report_to = \"wandb\",\n",
    "        run_name = f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"  # W&B è¿è¡Œåç§°ï¼ŒåŒ…å«å½“å‰æ—¶é—´æˆ³\n",
    "    ),\n",
    "    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False), # æ•°æ®æ•´ç†å™¨\n",
    ")\n",
    "\n",
    "model.config.use_cache = False # ç¦ç”¨ç¼“å­˜ä»¥é¿å…è­¦å‘Šã€‚æ¨ç†æ—¶è¯·é‡æ–°å¯ç”¨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b9046f7-94f6-4549-902a-f79741ea6f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 1:03:49, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.258400</td>\n",
       "      <td>0.391524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.342200</td>\n",
       "      <td>0.295344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.261100</td>\n",
       "      <td>0.240467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.232700</td>\n",
       "      <td>0.226517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.220600</td>\n",
       "      <td>0.217926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.209200</td>\n",
       "      <td>0.212170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>0.208268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.198200</td>\n",
       "      <td>0.206516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>0.204782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.204114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.33244283866882324, metrics={'train_runtime': 3845.3729, 'train_samples_per_second': 2.08, 'train_steps_per_second': 0.13, 'total_flos': 1.8495932347082342e+17, 'train_loss': 0.33244283866882324, 'epoch': 1.567398119122257})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() # å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30491e1-8ae8-4272-b7e4-b1653c86e948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39c7ab02-ff43-439f-a7f0-43fb4d30b677",
   "metadata": {},
   "source": [
    "## 8. åŸºäºbase modelå’ŒLoRA modelè¿›è¡Œæ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c98529a-3570-444e-9293-b6d42a1c1bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/test/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.14s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"/root/autodl-tmp/model/Meta-Llama-3-8B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token = True,\n",
    "    trust_remote_code = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d81137-62d5-479f-bf48-ac47350896e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# åŠ è½½ QLoRA adapter\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "best_qlora_checkpoint = \"/root/autodl-tmp/project/2_QLoRA/notebook/llama-3-8B-QLoRA/checkpoint-500\"\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model,\n",
    "                                     best_qlora_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8393e5cb-1740-4b9a-9d93-ecdb72823da0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "2024-05-24 11:05:37.558035: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-24 11:05:37.642115: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-24 11:05:38.944976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
      "This function should describe the target string accurately and the function must be one of the following ['inform','request', 'give_opinion', 'confirm','verify_attribute','suggest','request_explanation','recommend','request_attribute'].\n",
      "The attributes must be one of the following: ['name', 'exp_release_date','release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release','specifier']\n",
      "\n",
      "### Target sentence:\n",
      "Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?\n",
      "\n",
      "### Meaning representation:\n",
      "verify_attribute(name[Little Big Adventure], rating[average], has_multiplayer[no], platforms[PlayStation])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
    "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
    "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
    "\n",
    "### Target sentence:\n",
    "Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?\n",
    "\n",
    "### Meaning representation:\n",
    "\"\"\"\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2099eed5-42bd-4efa-bce9-a0c6ab82a608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
